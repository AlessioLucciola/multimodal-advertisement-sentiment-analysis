{"cells":[{"cell_type":"markdown","metadata":{"id":"NoLiiQ8zmWWr"},"source":["# Facial Expression Recognition\n","\n","* The data consists of 48x48 pixel grayscale images of faces. \n","* The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. \n","* The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories:\n","0. `Angry`\n","1. `Disgust` \n","2. `Fear` \n","3. `Happy`\n","4. `Sad`\n","5. `Surprise`,\n","6. `Neutral`"]},{"cell_type":"markdown","metadata":{"id":"Vt8WTC8sn-Ir"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T997E_F9JiIi","trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","\n","import os\n","import numpy as np \n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","\n","from torchsummary import summary\n","\n","from PIL import Image\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","import torchvision\n","import torchvision.transforms as T\n","from torchvision.utils import make_grid\n","\n","sns.set_style('whitegrid')\n","plt.style.use(\"fivethirtyeight\")\n","pd.set_option('display.max_columns', 20)\n","\n","\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"9LVj2dPamcZi"},"source":["# Dataset Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jxGt6zJ4KtzF","outputId":"fa518511-3d04-4590-d913-ee0fd9980216","trusted":true},"outputs":[],"source":["dataset = pd.read_csv('data/fer20131.csv')\n","dataset.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hgFlSQBZLAlT","trusted":true},"outputs":[],"source":["# dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JKkyJuFZLalo","outputId":"785c80a8-49fb-4dd4-ac79-1c6f20a5153a","trusted":true},"outputs":[],"source":["dataset.Usage.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"jqCAkFpQoLPN"},"source":["* We're going to use the `Training` and `PublicTest` rows combined together for training and validation set split into  80-20 proportion\n","* `PrivateTest` will be our final test dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ynvFOXLkLdr_","trusted":true},"outputs":[],"source":["# extracting pixel data from pixel column\n","# convert it to integer\n","# drop original pixel column\n","# add all pixels as individual column\n","\n","pixels = []\n","\n","for pix in dataset.pixels:\n","    values = [int(i) for i in pix.split()]\n","    pixels.append(values)\n","\n","pixels = np.array(pixels)\n","\n","# rescaling pixel values\n","pixels = pixels/255.0\n","\n","\n","dataset.drop(columns=['pixels'], axis=1, inplace=True)\n","\n","pix_cols = [] # for keeping track of column names\n","\n","# add each pixel value as a column\n","for i in range(pixels.shape[1]):\n","    name = f'pixel_{i}'\n","    pix_cols.append(name)\n","    dataset[name] = pixels[:, i]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dAJqGbezo1NK","outputId":"78db012c-8384-404d-eddf-7941c4e5f1d1","trusted":true},"outputs":[],"source":["dataset.head()"]},{"cell_type":"markdown","metadata":{"id":"wBTQTP_3LtGi"},"source":["# Dataset Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fu8Dkz_uTrLh","trusted":true},"outputs":[],"source":["emotions = {\n","    0: 'Angry', \n","    1: 'Disgust', \n","    2: 'Fear', \n","    3: 'Happy', \n","    4: 'Sad', \n","    5: 'Surprise', \n","    6: 'Neutral'\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8PR-K43mPMR","trusted":true},"outputs":[],"source":["class FERDataset(Dataset):\n","    '''\n","        Parse raw data to form a Dataset of (X, y).\n","    '''\n","    def __init__(self, df, transform=None):\n","        self.df = df\n","        self.transform = transform\n","        self.tensor_transform = T.ToTensor()\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx]\n","        img_id = int(row['emotion'])\n","        img = np.copy(row[pix_cols].values.reshape(48, 48))\n","        img.setflags(write=True)\n","\n","        if self.transform:\n","            img = Image.fromarray(img)\n","            img = self.transform(img)\n","        else:\n","            img = self.tensor_transform(img)\n","\n","        return img, img_id"]},{"cell_type":"markdown","metadata":{"id":"72jvNlUpcc3H"},"source":["# Data Imbalance\n","\n","* To deal with class Imbalance we can try different image transformations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qul49ZRsZir5","outputId":"50daaf4c-d141-4230-d545-13f4999ceb0a","trusted":true},"outputs":[],"source":["plt.figure(figsize=(9, 8))\n","sns.countplot(x=dataset.emotion)\n","_ = plt.title('Emotion Distribution')\n","_ = plt.xticks(ticks=range(0, 7), labels=[emotions[i] for i in range(0, 7)], )"]},{"cell_type":"markdown","metadata":{"id":"XeNbweU0m4Cy"},"source":["# Data Augmentations\n","\n","* We're going to apply various augmentation techniques.\n","* All available transformations are listed in : [pytorch transforms](https://pytorch.org/docs/stable/torchvision/transforms.html)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wo3OWqmgoTX0","trusted":true},"outputs":[],"source":["def image_transformations() -> (object, object):\n","    '''\n","        Return transformations to be applied.\n","        Input:\n","            None\n","        Output:\n","            train_tfms: transformations to be applied on the training set\n","            valid_tfms: transformations to be applied on the validation or test set\n","    '''\n","\n","    train_trans = [      \n","        T.RandomCrop(48, padding=4, padding_mode='reflect'),     \n","        T.RandomRotation(15),\n","        T.RandomAffine(\n","            degrees=0,\n","            translate=(0.01, 0.12),\n","            shear=(0.01, 0.03),\n","        ),\n","        T.RandomHorizontalFlip(),\n","        T.ToTensor(),\n","    ]\n","\n","    val_trans = [\n","        T.ToTensor(), \n","    ]\n","\n","    train_transformations = T.Compose(train_trans)\n","    valid_tfms = T.Compose(val_trans)\n","\n","    return train_transformations, valid_tfms"]},{"cell_type":"markdown","metadata":{"id":"N5BvCq13m6wa"},"source":["# Dataset and Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AK3PTjagQAgh","trusted":true},"outputs":[],"source":["def get_train_dataset(dataframe: object, transformation: bool=True) -> (object, object):\n","    '''\n","        Returns an object on FERDataset class\n","        Input:\n","            dataframe: object -> DataFrame object containing the whole data\n","            transformation: bool [optional] ->  Apply transformations\n","    '''\n","\n","    # extracts rows specific to Training, PublicTest\n","    dataframe = dataframe.loc[dataframe.Usage.isin(['Training', 'PublicTest'])]\n","    # drop Usage column as it's no longer needed    \n","    dataframe = dataframe.drop('Usage', axis=1)\n","\n","    # split dataset into training and validation set\n","    np.random.seed(42)  \n","    msk = np.random.rand(len(dataframe)) < 0.8\n","\n","    train_df = dataframe[msk].reset_index()\n","    val_df = dataframe[~msk].reset_index()\n","\n","    # get transformations\n","    if transformation:\n","        train_tfms, valid_tfms = image_transformations()\n","    else:\n","        train_tfms, valid_tfms = None, None\n","\n","    # fetch dataset\n","    train_ds = FERDataset(dataframe, transform=train_tfms)\n","    val_ds = FERDataset(dataframe, transform=valid_tfms)\n","    return train_ds, val_ds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gCWrF8EUR2R5","trusted":true},"outputs":[],"source":["def get_train_dataloader(dataframe: object, transformation=True, batch_size: int=64) -> (object, object):\n","    '''\n","        Returns train and test dataloaders.\n","        Input:\n","            dataframe: dataset DataFrame object\n","            batch_size: [optional] int\n","        Output:\n","            train_dl: train dataloader object\n","            valid_dl: validation dataloader object\n","    '''\n","    # fetech train and validation dataset\n","    train_ds, valid_ds = get_train_dataset(dataframe, transformation=transformation)\n","    \n","    train_dl = DataLoader(train_ds, batch_size, shuffle=True, \n","                     num_workers=3, pin_memory=True)\n","    valid_dl = DataLoader(valid_ds, batch_size*2, \n","                    num_workers=2, pin_memory=True)\n","    \n","    return train_dl, valid_dl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cd6CFvpjp529","trusted":true},"outputs":[],"source":["def get_test_dataloader(dataframe: object, batch_size: int=128) -> object:\n","    '''\n","        Returns test set dataloaders.\n","        Input:\n","            dataframe: dataset DataFrame object\n","            batch_size: [optional] int\n","        Output:\n","            test_dl: test dataloader object\n","    '''\n","    # extracts rows specific to PrivateTest\n","    test_df = dataframe.loc[dataset.Usage.isin(['PrivateTest'])]\n","\n","    # drop Usage column as it's no longer needed\n","    test_df = test_df.drop('Usage', axis=1)\n","\n","    # get transformations same as validation set\n","    _, valid_tfms = image_transformations()\n","    \n","    test_dataset = FERDataset(test_df, transform=valid_tfms)\n","    test_dl = DataLoader(test_dataset, batch_size, num_workers=3 , pin_memory=True)\n","\n","    # move loader to GPU (class defined ahead)\n","    test_dl = DeviceDataLoader(test_dl, device)\n","    return test_dl"]},{"cell_type":"markdown","metadata":{"id":"gUlACDVtXMbn"},"source":["# Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbJeCeU7rSpb","trusted":true},"outputs":[],"source":["train_dl_un, _ = get_train_dataloader(dataset, transformation=False)\n","train_dl, _ = get_train_dataloader(dataset)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"BalPL37ou3RO","outputId":"b19e8d4a-fd0b-4f83-e2cd-043f0f23b234","trusted":true},"outputs":[],"source":["for images, _ in train_dl_un:\n","    print('images.shape:', images.shape)\n","    plt.figure(figsize=(16, 8))\n","    plt.axis(\"off\")\n","    plt.imshow(make_grid(images, nrow=8).permute((1, 2, 0))) # move the channel dimension\n","    break\n","\n","_ = plt.suptitle(\"Images\", y=0.92, fontsize=16)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r6GRvUsWWEdU","outputId":"c96998d6-e936-4df4-a18e-75de1262f556","trusted":true},"outputs":[],"source":["for images, _ in train_dl:\n","    print('images.shape:', images.shape)\n","    plt.figure(figsize=(16, 8))\n","    plt.axis(\"off\")\n","    plt.imshow(make_grid(images, nrow=8).permute((1, 2, 0))) # move the channel dimension\n","    break\n","\n","_ = plt.suptitle(\"Transformed Images\", y=0.92, fontsize=16)"]},{"cell_type":"markdown","metadata":{"id":"kl9BPDAisCyY"},"source":["# Setting up GPU usage"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w3l45fBTsFtS","trusted":true},"outputs":[],"source":["def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","    \n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl: \n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cKfzxEqnsHAa","outputId":"16c5c1b4-57bf-4a0b-add7-eda4727097f3","trusted":true},"outputs":[],"source":["device = get_default_device()\n","device"]},{"cell_type":"markdown","metadata":{"id":"u3Kw9gHa1HpT"},"source":["# Model Building"]},{"cell_type":"markdown","metadata":{"id":"wwovvTy61c__"},"source":["## Base Image Classification Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nIZfIyWqb-gq","trusted":true},"outputs":[],"source":["# Can be used for any Image Classification task\n","\n","class ImageClassificationBase(nn.Module):\n","    def training_step(self, batch):\n","        inputs, labels = batch\n","        outputs = self(inputs)\n","        loss = F.cross_entropy(outputs, labels)\n","        acc = accuracy(outputs, labels)\n","        return {'loss': loss, 'acc': acc.detach()}\n","    \n","    def validation_step(self, batch):\n","        images, labels = batch \n","        out = self(images)                    # Generate predictions\n","        loss = F.cross_entropy(out, labels)   # Calculate loss\n","        acc = accuracy(out, labels)           # Calculate accuracy\n","        return {'val_loss': loss.detach(), 'val_acc': acc.detach()}\n","        \n","    def get_metrics_epoch_end(self, outputs, validation=True):\n","        if validation:\n","            loss_ = 'val_loss'\n","            acc_ = 'val_acc'\n","        else:\n","            loss_ = 'loss'\n","            acc_ = 'acc'\n","\n","        batch_losses = [x[f'{loss_}'] for x in outputs]\n","        epoch_loss = torch.stack(batch_losses).mean()   \n","        batch_accs = [x[f'{acc_}'] for x in outputs]\n","        epoch_acc = torch.stack(batch_accs).mean()      \n","        return {f'{loss_}': epoch_loss.detach().item(), f'{acc_}': epoch_acc.detach().item()}\n","    \n","    def epoch_end(self, epoch, result, num_epochs):\n","        print(f\"Epoch: {epoch+1}/{num_epochs} -> lr: {result['lrs'][-1]:.5f} loss: {result['loss']:.4f}, acc: {result['acc']:.4f}, val_loss: {result['val_loss']:.4f}, val_acc: {result['val_acc']:.4f}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"7H-YagmS1vtk"},"source":["## Metric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XN-m7L7i1u52","trusted":true},"outputs":[],"source":["def accuracy(outputs, labels):\n","    _, preds = torch.max(outputs, dim=1)\n","    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"]},{"cell_type":"markdown","metadata":{"id":"sTt5VSi-1y1x"},"source":["## Model: ResNet-9 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twFBQdsW1x-7","trusted":true},"outputs":[],"source":["def conv_block(in_channels, out_channels, pool=False):\n","    layers = [\n","              nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","              nn.BatchNorm2d(out_channels),\n","              nn.ReLU(inplace=True)\n","    ]\n","\n","    if pool:\n","        layers.append(nn.MaxPool2d(kernel_size=2))\n","    \n","    return nn.Sequential(*layers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IpDco92U2aeK","trusted":true},"outputs":[],"source":["# updated channels for the use case\n","# added and additional layer in classifier\n","\n","class ResNet9(ImageClassificationBase):\n","    def __init__(self, in_channels, num_classes):\n","        super().__init__()\n","        self.conv1 = conv_block(in_channels, 16, pool=False) # 16 x 48 x 48\n","        self.conv2 = conv_block(16, 32, pool=True) # 32 x 24 x 24\n","        self.res1 = nn.Sequential( #  32 x 24 x 24\n","            conv_block(32, 32, pool=False), \n","            conv_block(32, 32, pool=False)\n","        )\n","\n","        self.conv3 = conv_block(32, 64, pool=True) # 64 x 12 x 12\n","        self.conv4 = conv_block(64, 128, pool=True) # 128 x 6 x 6\n","\n","        self.res2 = nn.Sequential( # 128 x 6 x 6\n","             conv_block(128, 128), \n","             conv_block(128, 128)\n","        )\n","        \n","        self.classifier = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=2), # 128 x 3 x 3\n","            nn.Flatten(),\n","            nn.Linear(128*3*3, 512), #512\n","            nn.Linear(512, num_classes) # 7\n","        )\n","        self.network = nn.Sequential(\n","            self.conv1,\n","            self.conv2,\n","            self.res1,\n","            self.conv3,\n","            self.conv4,\n","            self.res2,\n","            self.classifier,\n","        )\n","\n","    def forward(self, xb):\n","        out = self.conv1(xb)\n","        out = self.conv2(out)\n","        out = self.res1(out) + out\n","        out = self.conv3(out)\n","        out = self.conv4(out)\n","        out = self.res2(out) + out\n","        out = self.classifier(out)\n","        return out       \n","\n","    def __repr__(self):\n","        return f\"{self.network}\"\n","    \n","    def __str__(self):\n","        summary(self.network, (1, 48, 48)) "]},{"cell_type":"markdown","metadata":{"id":"2-TaqJRk6dng"},"source":["## Model: From scratch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10FGaPFl2e_n","trusted":true},"outputs":[],"source":["class EmotionRecognition(ImageClassificationBase):\n","    def __init__(self):\n","        super().__init__()\n","        self.network = nn.Sequential(\n","            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(32),\n","            nn.MaxPool2d(2, 2), # output: 16 x 24 x 24\n","\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(64),\n","            nn.MaxPool2d(2, 2), # output: 64 x 12 x 12\n","\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(128),\n","            nn.MaxPool2d(2, 2), # output: 128 x 6 x 6\n","\n","            nn.Flatten(), \n","            nn.Linear(128*6*6, 1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","            nn.Linear(512, 7))\n","        \n","    def forward(self, xb):\n","        return self.network(xb)\n","\n","    def __repr__(self):\n","        return f\"{self.network}\"\n","    \n","    def __str__(self):\n","        summary(self.network, (1, 48, 48))"]},{"cell_type":"markdown","metadata":{"id":"iVXYiIcu-WFE"},"source":["# Setup Training"]},{"cell_type":"markdown","metadata":{"id":"c0aste1w-YVE"},"source":["## Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cQchFtLE8LW2","trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def evaluate(model: object, val_loader: object) -> dict:\n","    '''\n","        Evaluate model on the validation set\n","        Input:\n","            model: training model object\n","            val_loder: validation data loader object\n","        Output:\n","            validation metrics\n","    '''\n","\n","    model.eval()\n","    outputs = [model.validation_step(batch) for batch in val_loader]\n","    return model.get_metrics_epoch_end(outputs=outputs, validation=True)\n","\n","\n","def get_lr(optimizer: object) -> float:\n","    ''' Returns current learning rate'''\n","\n","    for param_group in optimizer.param_groups:\n","        return param_group['lr']\n","\n","\n","def fit_model(model_name: str,\n","              model: object, \n","              epochs: int, \n","              max_lr: float, \n","              train_loader: object, \n","              val_loader: object,\n","              weight_decay: float=0, \n","              grad_clip: float=None, \n","              opt_func: object=torch.optim.SGD):\n","    '''\n","        This function is responsible for training our model.\n","        We use a One Cycle learning rate policy to update our learning rate \n","        with each epoch.\n","        The best model is saved during each epoch.\n","        Input:\n","            model_name: str \n","            model: object\n","            epochs: int -> Max epochs\n","            max_lr: float -> Maximum allowed learning rate during learning\n","            train_loader: training set data loader\n","            val_loader: validation set data loader\n","            weight_decay: float -> value to decrease weights during training of each batch\n","            grad_clip: float -> maximum allowed gradient value\n","            opt_func: optimzer object\n","        Output:\n","            history: list of metrics\n","    '''\n","\n","    torch.cuda.empty_cache()\n","    BEST_VAL_SCORE = 0.0 # for keeping track of best model score\n","    history = []\n","\n","    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n","    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, max_lr=max_lr,\n","                                                    epochs=epochs, \n","                                                    steps_per_epoch=len(train_loader))\n","\n","    for epoch in range(epochs):\n","        train_history = []\n","        lrs = []\n","\n","        # Training Phase \n","        model.train()\n","        for batch in tqdm(train_loader, ascii=True, desc=f'Epoch: {epoch+1}/{epochs}'):\n","            info = model.training_step(batch)\n","            loss = info['loss']\n","            # contains batch loss and acc for training phase\n","            train_history.append(info)\n","            loss.backward()\n","\n","            # Gradient clipping\n","            if grad_clip:\n","                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n","\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            lrs.append(get_lr(optimizer))\n","            scheduler.step()\n","\n","\n","        train_result = model.get_metrics_epoch_end(train_history, validation=False)\n","        val_result = evaluate(model, val_loader)\n","        result = {**train_result, **val_result}\n","        result['lrs'] = lrs\n","        model.epoch_end(epoch, result, epochs)\n","\n","        # Save the best model\n","        if result['val_acc'] > BEST_VAL_SCORE:\n","            BEST_VAL_SCORE = result['val_acc']\n","            save_name = f\"{model_name}_epoch-{epoch+1}_score-{round(result['val_acc'], 4)}.pth\"\n","            !rm -f '{model_name}'_*\n","            torch.save(model.state_dict(), save_name)\n","\n","        history.append(result)\n","    return history"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"both","id":"RnhRaOI5DhsI","trusted":true},"outputs":[],"source":["# functions to fetch test dataset and generate submission file for best model\n","\n","def load_best(model_name: str) -> object:\n","    '''Returns the best model'''\n","\n","    # get model defintion\n","    best_model = models[model_name]\n","\n","    # load trained weights\n","    path = r\"./\"\n","    file_path = ''\n","    \n","    for i in os.listdir(path):\n","        if os.path.isfile(os.path.join(path,i)) and i.startswith(f'{model_name}'):\n","            file_path = os.path.join(path, i)\n","            \n","    print(f\"Loaded model: {file_path[2:]} weights.\")\n","    best_model.load_state_dict(torch.load(file_path))\n","\n","    # move model to gpu\n","    best_model = to_device(best_model, device)\n","    return best_model   \n","\n","\n","@torch.no_grad()\n","def generate_prediction(model_name: str) -> None:\n","    '''Generate prediction on the test set'''\n","\n","    # load test dataset\n","    test_dl = get_test_dataloader(dataset)\n","    \n","    # load model\n","    model = load_best(model_name)\n","\n","    # clear cuda cache\n","    torch.cuda.empty_cache()\n","\n","    # generate prediction using the validation step method defined in Base class\n","    with torch.no_grad():\n","        model.eval()\n","        outputs = [model.validation_step(batch) for batch in test_dl]\n","        metrics = model.get_metrics_epoch_end(outputs=outputs, validation=True)\n","\n","    print(f\"Test Scores:\\n Loss: {round(metrics['val_loss'], 3)}, Accuracy: {round(metrics['val_acc'], 3)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-KouicV_sTy","trusted":true},"outputs":[],"source":["def end_to_end(model_name: str, parameters: dict=None) -> dict:\n","    '''\n","        A simple function end-to-end training and testing on the selected model.\n","        Inputs:\n","            model_name: str -> chosen model name\n","            parameters: dict -> dictionary of hyperparameters for the model\n","        Outputs:\n","            history: dict -> dictionary containing model metrics(loss, score, lr)\n","\n","    '''\n","    torch.cuda.empty_cache()\n","\n","    # hyperparameters\n","    BATCH_SIZE = 512 # batch_sizes[model_name]\n","    epochs = parameters[\"epochs\"]\n","    max_lr = parameters[\"max_lr\"]\n","    weight_decay = parameters[\"weight_decay\"]\n","    grad_clip = parameters[\"grad_clip\"]\n","    opt_func = parameters[\"opt_func\"]\n","\n","    # get transformed dataset\n","    train_dl, valid_dl = get_train_dataloader(dataset, batch_size=BATCH_SIZE)\n","    # move dataset to use GPU\n","    train_dl = DeviceDataLoader(train_dl, device)\n","    valid_dl = DeviceDataLoader(valid_dl, device)\n","\n","    # get model\n","    model = models[model_name]\n","\n","    # move model to GPU\n","    model = to_device(model, device)\n","    \n","    # train model\n","    history = fit_model(\n","                model_name,\n","                model, \n","                epochs, \n","                max_lr, \n","                train_dl, \n","                valid_dl,\n","                weight_decay, \n","                grad_clip, \n","                opt_func\n","            )\n","\n","    # cleaning\n","    torch.cuda.empty_cache()\n","\n","    # generate predictions\n","    print(\"Genearating predictions on the Test set\")\n","    generate_prediction(model_name)\n","    return history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-ksVBp3I2B3","trusted":true},"outputs":[],"source":["# plotting metrics\n","\n","def plot_accuracies(history):\n","    train_acc = [r['acc'] for r in history]\n","    val_acc = [r['val_acc'] for r in history]\n","    plt.plot(train_acc, '-kx', label=\"train_acc\")\n","    plt.plot(val_acc, '-rx', label=\"val_acc\")\n","    plt.legend()\n","    _ = plt.xticks(ticks=range(len(train_acc)), \n","                   labels=[str(i) for i in range(1, len(train_acc)+1)])\n","    plt.xlabel('epoch')\n","    plt.ylabel('Accuracy')\n","    plt.title('Accuracy vs. epochs')\n","\n","def plot_losses(history):\n","    train_losses = [r['loss'] for r in history]\n","    val_losses = [r['val_loss'] for r in history]\n","    plt.plot(train_losses, '-kx', label=\"train_loss\")\n","    plt.plot(val_losses, '-rx', label=\"val_loss\")\n","    plt.legend()\n","    _ = plt.xticks(ticks=range(len(train_losses)), \n","                   labels=[str(i) for i in range(1, len(train_losses)+1)])\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.title('Loss vs. epochs')\n","\n","def plot_lrs(history):\n","    lrs = np.concatenate([x.get('lrs', []) for x in history])\n","    plt.plot(lrs)\n","    plt.xlabel('Batch no.')\n","    plt.ylabel('Learning rate')\n","    plt.title('Learning Rate vs. Batch no.');"]},{"cell_type":"markdown","metadata":{"id":"oCusc8giyQ3r"},"source":["## Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8N1BMlbICHQf","trusted":true},"outputs":[],"source":["models = {\n","    'ResNet9': ResNet9(in_channels=1, num_classes=7),\n","    'EmotionRecognition': EmotionRecognition(),\n","}"]},{"cell_type":"markdown","metadata":{"id":"-cb06_CDHWVv"},"source":["# Train Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FKxBoYgOGIC-","trusted":true},"outputs":[],"source":["# TRAINING CONSTANTS\n","\n","training_parameters = {\n","    \"epochs\": 2,\n","    \"max_lr\": 0.01,\n","    \"weight_decay\": 0.1,\n","    \"grad_clip\": 1e-4,\n","    \"opt_func\": torch.optim.Adam,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOS-8EmPHnGz","outputId":"4a49587f-b4f0-40e3-ec59-53054840d52e","trusted":true},"outputs":[],"source":["model_name = \"ResNet9\"\n","# model_name = \"EmotionRecognition\"\n","\n","history = end_to_end(model_name, training_parameters)"]},{"cell_type":"markdown","metadata":{"id":"fE27BEoVJLB5"},"source":["# Training plots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3WagEoB1JSfV","outputId":"3a1be011-92ed-4dc8-c196-e159a155d03a","trusted":true},"outputs":[],"source":["# plotting score and loss\n","\n","plt.figure(figsize=(18, 6))\n","plt.subplot(1, 3, 1)\n","plot_accuracies(history)\n","plt.subplot(1, 3, 2)\n","plot_losses(history)\n","\n","plt.subplot(1, 3, 3)\n","plot_lrs(history)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
