{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # Add the root folder to the sys.path\n",
    "\n",
    "from config import *\n",
    "from shared.constants import general_emotion_mapping\n",
    "from dataloaders.video_custom_dataloader import video_custom_dataloader\n",
    "\n",
    "# Reload the configuration\n",
    "from importlib import reload\n",
    "reload(sys.modules['config']) \n",
    "\n",
    "from config import *\n",
    "from shared.constants import general_emotion_mapping\n",
    "from dataloaders.video_custom_dataloader import video_custom_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Dataloader-- Using positive/negative labels mapping.\n",
      "--Dataloader-- Splitting the dataset WITH overlapping between subjects\n",
      "--- Train subjects: [20  5 17 21  6 23 10 22  3  7 18  8  0  4 19 12 11  2 15  1  9 13 14 16] \n",
      "--- Validation subjects: [19 23  7  3 21  0  2 18  6  4  5 10  9  1 16 22 12 14 11  8 20 17 15 13] \n",
      "--- Test subjects: [ 9 21  2 22  6  4 19 23  8 12 17 20 16  1 10  7 13 11 18  3 15 14  0  5]\n"
     ]
    }
   ],
   "source": [
    "custom_dataloader = video_custom_dataloader(csv_original_files=\"../\"+VIDEO_METADATA_CSV,\n",
    "                                   csv_frames_files=\"../\"+VIDEO_METADATA_FRAMES_CSV,\n",
    "                                   batch_size=BATCH_SIZE,\n",
    "                                   frames_dir=\"../\"+FRAMES_FILES_DIR,\n",
    "                                   seed=RANDOM_SEED,\n",
    "                                   limit=LIMIT,\n",
    "                                   preload_frames=PRELOAD_FRAMES,\n",
    "                                   apply_transformations=APPLY_TRANSFORMATIONS,\n",
    "                                   balance_dataset=BALANCE_DATASET,\n",
    "                                   normalize=NORMALIZE,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danil\\Documents\\Repositories\\MI\\utils\\..\\datasets\\video_custom_dataset.py:116: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data.fillna({\"balanced\": False}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Data Balance-- balance_data set to True. Training data will be balanced.\n",
      "--Data Balance-- Classes before balancing:  {6: 5094, 4: 4951, 1: 4866, 2: 4601, 3: 4466, 5: 4333, 7: 4298, 0: 2157}\n",
      "--Data Balance-- The most common class is 6 with 5094 images.\n",
      "--Data Balance (Oversampling)-- Adding 2937 to 0 class..\n",
      "--Data Balance (Oversampling)-- Adding 228 to 1 class..\n",
      "--Data Balance (Oversampling)-- Adding 493 to 2 class..\n",
      "--Data Balance (Oversampling)-- Adding 628 to 3 class..\n",
      "--Data Balance (Oversampling)-- Adding 143 to 4 class..\n",
      "--Data Balance (Oversampling)-- Adding 761 to 5 class..\n",
      "--Data Balance (Oversampling)-- Adding 796 to 7 class..\n",
      "--Data Balance-- Classes after balancing:  {0: 5094, 1: 5094, 2: 5094, 3: 5094, 4: 5094, 5: 5094, 6: 5094, 7: 5094}\n",
      "--Data Preloading-- Preloading frames files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40752/40752 [02:21<00:00, 288.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Dataset-- Train dataset size: 40752\n",
      "--Data Preloading-- Preloading frames files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2933/4347 [00:06<00:03, 427.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m custom_dataloader\u001b[38;5;241m.\u001b[39mget_train_dataloader()\n\u001b[1;32m----> 2\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_dataloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_val_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m custom_dataloader\u001b[38;5;241m.\u001b[39mget_test_dataloader()\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Repositories\\MI\\utils\\..\\dataloaders\\video_custom_dataloader.py:128\u001b[0m, in \u001b[0;36mvideo_custom_dataloader.get_val_dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_val_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 128\u001b[0m     val_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_custom_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mfiles_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframes_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mis_train_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mpreload_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreload_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mbalance_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbalance_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mapply_transformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_transformations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--Dataset-- Validation dataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_dataset\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Repositories\\MI\\utils\\..\\datasets\\video_custom_dataset.py:39\u001b[0m, in \u001b[0;36mvideo_custom_dataset.__init__\u001b[1;34m(self, data, files_dir, is_train_dataset, preload_frames, balance_dataset, apply_transformations, normalize)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Preload frames files\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreload_frames_files:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_frames_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Repositories\\MI\\utils\\..\\datasets\\video_custom_dataset.py:80\u001b[0m, in \u001b[0;36mvideo_custom_dataset.read_frames_files\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m frames \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_name \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m---> 80\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     frames[frame_name] \u001b[38;5;241m=\u001b[39m frame\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frames\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Repositories\\MI\\utils\\..\\datasets\\video_custom_dataset.py:72\u001b[0m, in \u001b[0;36mvideo_custom_dataset.get_frame\u001b[1;34m(self, frame_name)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(frame_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     71\u001b[0m     frame \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m---> 72\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Convert to RGB\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frame\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Repositories\\MI\\.venv\\Lib\\site-packages\\PIL\\Image.py:922\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    876\u001b[0m ):\n\u001b[0;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    924\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Repositories\\MI\\.venv\\Lib\\site-packages\\PIL\\ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader = custom_dataloader.get_train_dataloader()\n",
    "val_loader = custom_dataloader.get_val_dataloader()\n",
    "test_loader = custom_dataloader.get_test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_grid(frames: torch.Tensor, labels: torch.Tensor,title=None):\n",
    "    # Convert from tensor to image format\n",
    "    frames = frames.numpy()\n",
    "    labels = labels.numpy() \n",
    "\n",
    "    # Choose 10 random images to display from the batch with their corresponding labels\n",
    "    mapped_labels = [general_emotion_mapping[label.item()] for label in labels]\n",
    "\n",
    "    fig, axs = plt.subplots(2, 5, figsize=(20, 10))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(5):\n",
    "            axs[i, j].imshow(frames[5*i+j].transpose(1, 2, 0))\n",
    "            axs[i, j].set_title(mapped_labels[5*i+j])\n",
    "            axs[i, j].axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, batch = next(enumerate(train_loader))\n",
    "frames, labels = batch['frame'], batch['emotion']\n",
    "plot_image_grid(frames, labels,\"Train images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, batch = next(enumerate(val_loader))\n",
    "frames, labels = batch['frame'], batch['emotion']\n",
    "plot_image_grid(frames, labels,\"Val images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, batch = next(enumerate(test_loader))\n",
    "frames, labels = batch['frame'], batch['emotion']\n",
    "plot_image_grid(frames, labels,\"Test images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
